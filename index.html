<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompts & Provenance | Introduction</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <!-- Small header -->
    <header class="small-header">
        <a href="https://www.intermediations.xyz">iNTERMEDIATIONS</a>
    </header>

    <!-- Landing section wrapper -->
    <div class="landing-section">
        <!-- Title content -->
        <div class="static-content">
            <div class="title-row-landing">
                <h1>
                    <span>Prompts & Provenance:</span>
                    <span>Re-situating Place in</span>
                    <span>Text-to-Image Models</span>
                </h1>
                <h1>
                    <ul>
                        <li><a href="about.html">ABOUT : PROMPTS & PROVENANCE</a></li>
                        <li><a href="https://www.labiennale.org/en/news/biennale-architettura-2025-intelligens-natural-artificial-collective" target="_blank">@ BIENNALE D'ARCHITETTURA 2025</a></li>
                        <li><a href="https://www.intermediations.xyz" target="_blank">BY iNTERMEDIATIONS</a></li>
                    </ul>
                </h1>
            </div>
        </div>

        <!-- Grid container -->
        <div class="grid-container active">
            <!-- Grid items will be loaded here -->
        </div>

    </div>

    <!-- Separator -->
    <div class="content-separator"></div>
    
    <!-- Rest of the content -->
    <div class="static-content">
        <div class="title-row">
            <h3>
                <p>The representational power of synthetic images in Text-to-image models —computationally aggregated approximations of real sites—offers unprecedented opportunities for collective imagination. However, the dramatic proliferation of AI-generated imagery raises critical concerns about the authenticity and fidelity of computationally-led representations of place.</p>
            </h3>
            <h3>
                <br>
                <br>
                <span>If an estimated</span>
                <span>90%</span>
                <span>of images online</span>
                <span>may be AI-generated</span>
                <span>by 2026<sup class="footnote">1</sup></span>
                <span>How do current GenAI models perpetuate biased representations of place?</span>
            </h3>

        <div class="intro-text">
            <h3 class="section-title">
                <span>Currently:</span> 
                <span>34 million AI images<sup class="footnote">2</sup></span>
                <span>are created daily</span>
            </h3>
            <p>While extensive research on AI bias has focused on human representation—how systems depict different ethnicities, genders, and cultures—there has been little attention paid to how text-to-image models represent place & locality: the built environments and landscapes that shape our understanding of different cultures and communities.</p>
        </div>

        <div class="center-image" style="text-align: center; margin: 2rem auto;">
            <img src="_images/P&P_Rome Presentation_13.png" alt="Place Image" style="max-width: 100%; height: auto;">
        </div>
        <div class="intro-text">
            <p>Of concern are the implications for regions underrepresented in the underlying datasets fueling these AI systems—particularly in the "global south". The images of localities conjured by the mere usage of "global south" implies a texture of neglect and equality in carrying a seat within the global conversations within today's datasets.</p>
        </div>

        <!-- Optimized animation iframe container -->
        <div class="animation-container" style="width: 100%; height: 120vh; margin: 4rem 0; overflow: hidden;">
            <iframe 
                src="pp_animation.html" 
                style="width: 100%; height: 100%; border: none; transform: scale(0.9); transform-origin: center center;" 
                title="Layer Comparison Animation"
                loading="lazy"
                scrolling="no"
            ></iframe>
        </div>

        <div class="intro-text">
            <p>Our work examines how synthetic place generation affects our understanding and representation of global localities. This shift from studying bias against people to focusing on the compression of place offers complementary insights into how AI models construct and represent our physical world. Here, we compare the treatment of two mechanically-captured images (photographs) index the personal vision of Judd's Mother in her home country of Guyana and abroad on a visit in France.</p>
        </div>

        <div class="image-comparison-container">
            <div class="comparison-image">
                <img src="Case_Study_Biennale_Panels/P2P_Guyana_9827_High-res.png" alt="Guyana Place Image">
            </div>
            <div class="comparison-image">
                <img src="Case_Study_Biennale_Panels/P2P_France_9827_High-res.png" alt="France Place Image">
            </div>
        </div>

        <div class="intro-text">
            <p>Using a multimodal model, computer vision devoid of personal and cultural context only sees "a woman in a colorful shawl" by the "Arc de Triomphe" or "two women… at what appears to be Stabroek Market in Georgetown, Guyana." Tokenization fragments the semiotic connections into unbound radicals that are easily reassembled into distant interpretations. The model's latent semantic space draws its own cartography of meaning to map tokens into a semantic space where synthetic grafting begins—it is here that Georgetown, Guyana suddenly relates to Georgetown University and the Potomac River in Washington DC. Because instances of Paris' associative meaning are over-indexed, its semantic space is restricted within France. Decoding then faithfully reproduces these instances and in the process splices and appends well-indexed representations to their under-represented counterparts — the "colorful shawl" becomes the "exotic fabric" worn by a travel influencer. The final output we collectively see erases the personal vision, leaving us only with the synthetic simulacra of cultural relevance of place.</p>
        </div>

        <div class="intro-text">
            <p>In the age of AI, we must stem misrepresentation in synthetic imagery, and provoke ethical tools that protect and celebrate the nuanced poetics of locality.</p>
        </div>


        <div class="new-section semantic-sight-section">
            <div class="section-content">
                <h2 class="section-title">
                    <span>SEMANTIC NETWORK</span>
                </h2>
                <div class="network-container">
                    <iframe 
                        src="PP_Network/graph.html" 
                        frameborder="0" 
                        class="network-frame"
                        title="Semantic Sight Network Visualization">
                    </iframe>
                </div>
            </div>
        </div>
    </div>
    <div class="case-study-section">
        <h2 class="section-title">Venice, Italy & Ganvie, Benin</h2>
        
        <div class="case-study-images">
            <div class="case-study-item">
                <div class="center-image">
                    <img src="_images/P&P_Rome Presentation_18.png" alt="Comparative Analysis of Venice, Italy">
                </div>
            </div>

            <div class="case-study-item">
                <div class="center-image">
                    <img src="_images/P&P_Rome Presentation_19.png" alt="Comparative Analysis of Ganvie, Benin">
                </div>
            </div>
        </div>
    </div>
    
    <div class="methodology-section">
        <h2 class="section-title">Methodology</h2>
        
        <div class="methodology-content">
            <div class="methodology-details">
                <p>48 global destinations</p>
                <p>24 Global North / 24 Global South</p>
                <p>StabilityAI_Stable_Diffusion_3.5_Large</p>
                <p>Wikimedia Commons as ground truth</p>
            </div>

            <div class="methodology-highlights">
                <ul>
                    <li>First systematic study of place-based bias in AI</li>
                    <li>Global North/South comparative analysis</li>
                    <li>Novel methodology combining GIS + AI</li>
                    <li>Focus on built environment & cultural spaces</li>
                </ul>
            </div>

            <div class="acknowledgments">
                <p>Special thanks to Johan Michalove<a href="https://micha.love/2024/07/19/semioscape.html" target="_blank">Semioscape.org</a> for providing the "concept cartographer" tool used in our semantic network research and visualization development.</p>
            </div>

            <div class="footnotes">
                <p class="footnote-item">
                    <sup>1</sup> Europol. (2024). "Facing Reality: Law enforcement and the challenge of deepfakes." 
                    <a href="https://www.europol.europa.eu/publications-events/publications/facing-reality-law-enforcement-and-challenge-of-deepfakes#downloads" target="_blank">Source</a>
                </p>
                <p class="footnote-item">
                    <sup>3</sup> Stability AI. (2024). "Stable Diffusion 3.5 Large." 
                    <a href="https://huggingface.co/stabilityai/stable-diffusion-3.5-large" target="_blank">Source</a>
                </p>
                <p class="footnote-item">
                    <sup>4</sup> Designboom. (2023). "AI has generated 150 years worth of images in less than 12 months, study shows."
                    <a href="https://www.designboom.com/technology/ai-has-generated-150-years-worth-of-photographs-in-less-than-12-months-study-shows-08-21-2023/" target="_blank">Source</a>
                </p>
            </div>
        </div>

    </div>

     <!-- Footer within landing section -->
     <footer class="footer">
        <nav class="footer-nav">
            <ul>
                <li><a href="about.html">ABOUT : PROMPTS & PROVENANCE</a></li>
                <li><a href="https://www.labiennale.org/en/news/biennale-architettura-2025-intelligens-natural-artificial-collective" target="_blank">@ BIENNALE D'ARCHITETTURA 2025</a></li>
                <li><a href="https://www.intermediations.xyz" target="_blank">BY iNTERMEDIATIONS</a></li>
            </ul>
        </nav>
    </footer>

</div>

<script src="grid-interaction.js"></script>
<script>
function toggleMethodology() {
    const content = document.getElementById('methodologyContent');
    const icon = document.querySelector('.expand-icon');
    
    if (content.style.display === 'none') {
        content.style.display = 'flex';
        icon.textContent = '×';
        icon.classList.add('active');
    } else {
        content.style.display = 'none';
        icon.textContent = '+';
        icon.classList.remove('active');
    }
}

function toggleCaseStudy() {
    const content = document.getElementById('caseStudyContent');
    const icon = document.querySelector('.collapsible:nth-of-type(2) .expand-icon');
    
    if (content.style.display === 'none') {
        content.style.display = 'flex';
        icon.textContent = '×';
        icon.classList.add('active');
    } else {
        content.style.display = 'none';
        icon.textContent = '+';
        icon.classList.remove('active');
    }
}

const loadGridImages = async () => {
    const gridContainer = document.querySelector('.grid-container');
    
    // Clear existing content
    gridContainer.innerHTML = '';
    
    // Get all image files from the Gen_Images_Animation folder
    const response = await fetch('Gen_Images_Animation/');
    const files = await response.text();
    const parser = new DOMParser();
    const doc = parser.parseFromString(files, 'text/html');
    const links = Array.from(doc.querySelectorAll('a'))
        .filter(link => link.href.match(/\.(jpg|jpeg|png|gif)$/i));
    
    // Create grid items for each image
    links.forEach(link => {
        const fileName = link.href.split('/').pop();
        const locationName = decodeURIComponent(fileName.split('.')[0])
            .replace(/_/g, ' ')
            .replace(/([A-Z])/g, ' $1')
            .replace(/\s+/g, ' ') // Remove extra spaces
            .trim();
        
        const gridItem = document.createElement('div');
        gridItem.className = 'grid-item';
        
        gridItem.innerHTML = `
            <img src="Gen_Images_Animation/${fileName}" alt="${locationName}">
            <div class="grid-caption">
                <div class="location-caption">${locationName}</div>
            </div>
        `;
        
        gridContainer.appendChild(gridItem);
    });
};

// Load images when the page loads
window.addEventListener('load', loadGridImages);
</script>
</body>
</html>
